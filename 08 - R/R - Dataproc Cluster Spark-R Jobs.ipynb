{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8f0b3a",
   "metadata": {},
   "source": [
    "![ga4](https://www.google-analytics.com/collect?v=2&tid=G-6VDTYWLKX6&cid=1&en=page_view&sid=1&dl=statmike%2Fvertex-ai-mlops%2F08+-+R&dt=R+-+Dataproc+Cluster+Spark-R+Jobs.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/08%20-%20R/R%20-%20Dataproc%20Cluster%20Spark-R%20Jobs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A//raw.githubusercontent.com/statmike/vertex-ai-mlops/main/08%20-%20R/R%20-%20Dataproc%20Cluster%20Spark-R%20Jobs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/R%20-%20Dataproc%20Cluster%20Spark-R%20Jobs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A//raw.githubusercontent.com/statmike/vertex-ai-mlops/main/08%20-%20R/R%20-%20Dataproc%20Cluster%20Spark-R%20Jobs.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b07c1-4cd3-43e3-8639-feb554954981",
   "metadata": {},
   "source": [
    "# R - Dataproc Cluster Spark-R Jobs\n",
    "\n",
    "Running an **R** script as a job using [SparkR](https://spark.apache.org/docs/latest/sparkr.html#overview).  Submit a prepared script to a Google Cloud [Dataproc](https://cloud.google.com/dataproc/docs/concepts/overview) cluster as a job.  A cluster can be started up in 90s.\n",
    "\n",
    "> For a serverless approach to submitting a job check out the other workflow in this series:\n",
    ">- [R - Dataproc Serverless Spark-R Jobs](./R%20-%20Dataproc%20Serverless%20Spark-R%20Jobs.ipynb)\n",
    "\n",
    "---\n",
    "Part of the series of [**R**](https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/readme.md) workflows:\n",
    "\n",
    "A series of workflows focused on using **R** in Vertex AI as well as other Google Cloud services to run R code, train models with R, and serve predictionns with R.\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- This notebook running in Vertex AI Workbench Instance as described in the series [readme](./readme.md)\n",
    "- Run the workflow: [R - Notebook Based Workflow](./R%20-%20Notebook%20Based%20Workflow.ipynb)\n",
    "    - This prepares the data source used by the custom job in this workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5ded4-f3e0-4a38-86ed-e2d72b8b5ad0",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb954d6-3125-4d53-9661-ed5b29286084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.storage', 'google-cloud-storage'),\n",
    "    ('google.cloud.dataproc', 'google-cloud-dataproc')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec3e49-a595-4117-977b-316494d42b32",
   "metadata": {},
   "source": [
    "### Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c59951-6afd-4284-89fd-78d0536f510a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable dataproc.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782cf53-6826-4b82-8170-b3c1189c84c0",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2789c9f1-3540-4825-8046-efd9d9439ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1d9c6-02d5-4066-aa31-a7916b034b57",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c56f9d-485a-49d9-85bb-2bf0eacf7281",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65859277-1551-49e7-b912-38cd224b7d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a2c321-3a99-467e-b2d8-fc686e01426b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'dataproc-cluster'\n",
    "SERIES = 'r'\n",
    "\n",
    "# BigQuery Parameters\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES\n",
    "BQ_TABLE = 'bigquery-data'\n",
    "BQ_REGION = REGION[0:2]\n",
    "\n",
    "# GCS Parameters: Give bucket name\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "\n",
    "# key columns in the data:\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = ['transaction_id', 'splits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a842a9c-26ca-4948-b14c-a631f40b649c",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e151c0c-8b12-4ad3-af03-688476a2f0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import dataproc_v1\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba93609-23f5-43f4-af43-d39dfa260370",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbe1d619-6da0-42fe-9acf-829798a2cedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355ea24-5da8-4e09-910c-e108510b299c",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6b3114-56ef-4b0a-b992-90992c16a1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "dataproc_cluster = dataproc_v1.ClusterControllerClient(client_options = dict(api_endpoint = f\"{REGION}-dataproc.googleapis.com:443\"))\n",
    "dataproc_job = dataproc_v1.JobControllerClient(client_options = dict(api_endpoint = f\"{REGION}-dataproc.googleapis.com:443\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56579b2a-0ef3-4fd0-95aa-bde757f30e8d",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Training Code: **SparkR** Script\n",
    "\n",
    "The prior workflow in this series, [R - Notebook Based Workflow](./R%20-%20Notebook%20Based%20Workflow.ipynb), did the model training work in a notebook using an **R** kernel.  \n",
    "\n",
    "The first step is converting the workflow of the prior notebook to a script that will run with SparkR. The steps from the notebook workflow have been replicated in the **R** script included with this repository.  The cell below loads and shows this script.  \n",
    "- review directly in GitHub with [this link](https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/code/sparkr.R)\n",
    "\n",
    "**Notes On Script**\n",
    "- The steps are replicated identically with the following additions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffaaf972-36f8-4a20-a75d-70fae390dc03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```R\n",
       "\n",
       "n <- 1000000  # Number of random points\n",
       "x <- runif(n, -1, 1)\n",
       "y <- runif(n, -1, 1)\n",
       "\n",
       "inside <- x^2 + y^2 <= 1  # Points within the unit circle\n",
       "pi_estimate <- 4 * sum(inside) / n \n",
       "print(pi_estimate)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a view the script:\n",
    "SCRIPT_PATH = './code/sparkr.R'\n",
    "\n",
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```R\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9a8a5-eb1a-41f6-b30b-f5eaad632ede",
   "metadata": {},
   "source": [
    "---\n",
    "## Run SparkR Job On Dataproc Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512ff03-aff2-4275-b71d-8c263aef4cfe",
   "metadata": {
    "id": "1479c476-7755-4a7a-bddc-961e6570cea1"
   },
   "source": [
    "### Setup Dataproc\n",
    "\n",
    "- Network Configuration: https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58772c9c-dc60-4e2c-9751-bf5e892b5328",
   "metadata": {},
   "source": [
    "Current networks name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11464a2-e00a-46aa-a996-6c42dc73842c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4\n",
      "default  AUTO         REGIONAL\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute networks list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a243356-ded6-44f6-ae9b-7fa377e5beab",
   "metadata": {},
   "source": [
    "Open subnet connectivity to allow ingress communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af284ae-6eeb-4916-85f4-5691981f7d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating firewall...failed.                                                    \n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.compute.firewall-rules.create) Could not fetch resource:\n",
      " - The resource 'projects/statmike-mlops-349915/global/firewalls/allow-internal-ingress' already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute firewall-rules create allow-internal-ingress \\\n",
    "--network=default \\\n",
    "--source-ranges=10.128.0.0/9 \\\n",
    "--direction=ingress \\\n",
    "--action=allow \\\n",
    "--rules=all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264de46-f395-4918-860a-e9394d0962f3",
   "metadata": {},
   "source": [
    "### Create A Dataproc Cluster\n",
    "\n",
    "A cluster can be create using the `gcloud` CLI, REST, Console, or clients in a number of languages, like [here with Python](https://cloud.google.com/python/docs/reference/dataproc/latest).  [Reference](https://cloud.google.com/dataproc/docs/guides/create-cluster#dataproc-create-cluster-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd967a81-b1fa-4592-9240-b95848060310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_specs = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    cluster_name = f'{SERIES}-{EXPERIMENT}',\n",
    "    config = dict(\n",
    "        master_config = dict(num_instances = 1, machine_type_uri = 'n1-standard-2'),\n",
    "        worker_config = dict(num_instances = 3, machine_type_uri = 'n1-standard-2')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee8457e9-9b51-455f-8ac3-acf6a36c81ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = dataproc_cluster.create_cluster(\n",
    "    project_id = PROJECT_ID,\n",
    "    region = REGION,\n",
    "    cluster = cluster_specs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6f76a8e-5667-4c93-be67-b62eb35aefec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = cluster.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d072005c-116f-4165-a5bd-47c813a7bb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r-dataproc-cluster'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62cdcaef-7dc4-49ec-951f-b633475975d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Dataproc cluster in the console at this link:\n",
      "https://console.cloud.google.com/dataproc/clusters/r-dataproc-cluster/monitoring?region=us-central1&project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review the Dataproc cluster in the console at this link:\\nhttps://console.cloud.google.com/dataproc/clusters/{result.cluster_name}/monitoring?region={REGION}&project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc5a4f-e47f-4950-b8d0-a034db1ec4cd",
   "metadata": {},
   "source": [
    "### Copy Script To GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcba9a52-ac1f-4333-918b-4116ab714bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(GCS_BUCKET)\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63967f12-dfe5-4fb2-b6be-0387bdb6f52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/sparkr.R')\n",
    "blob.upload_from_filename(SCRIPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d02d2c3c-0212-416f-814a-f9805676f18f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r/dataproc-cluster/models/20240129004317/sparkr.R'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20b5ca-14b3-40fe-8060-2ee177df0d09",
   "metadata": {},
   "source": [
    "### Submit Job\n",
    "\n",
    "The [script can be submitted](https://cloud.google.com/dataproc/docs/guides/submit-job) with Google Cloud Console, the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/spark-r) or [one of the APIs](https://cloud.google.com/dataproc/docs/reference) including the [Python Client](https://cloud.google.com/python/docs/reference/dataproc/latest) used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad2e9ab4-2f42-444d-a58b-4c4f3753b550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_specs = dict(\n",
    "    placement = dict(cluster_name = cluster_specs['cluster_name']),\n",
    "    spark_r_job = dict(\n",
    "        main_r_file_uri = f'gs://{GCS_BUCKET}/{blob.name}',\n",
    "        args = ['1000']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34dc8434-e984-48f4-a3d3-426c2423577d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = dataproc_job.submit_job(\n",
    "    project_id = PROJECT_ID,\n",
    "    region = REGION,\n",
    "    job = job_specs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62ef878d-68f1-4b7d-a5cc-c0e09509ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5ec7978b-37de-4b71-9cce-b500d9913ba2'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.reference.job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881392d-be67-4ff8-a6ac-0c3eaae5e57e",
   "metadata": {},
   "source": [
    "### Wait On Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b88c557-c274-4269-aff9-7a78005e2a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    getjob = dataproc_job.get_job(project_id = PROJECT_ID, region = REGION, job_id = job.reference.job_id)\n",
    "    if getjob.status.state.name == \"ERROR\":\n",
    "        raise Exception(getjob.status.details)\n",
    "    elif getjob.status.state.name == \"DONE\":\n",
    "        print (\"Finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cf319a3-8484-4763-8ff8-a6d7be2577ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a9be452-df38-494a-8ce7-29d20800d4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review job details in the console at this link:\n",
      "https://console.cloud.google.com/dataproc/jobs/5ec7978b-37de-4b71-9cce-b500d9913ba2/monitoring?region=us-central1&project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review job details in the console at this link:\\nhttps://console.cloud.google.com/dataproc/jobs/{job.reference.job_id}/monitoring?region={REGION}&project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d60ce-4f8c-4fb6-8eac-093d3b0ea6ac",
   "metadata": {},
   "source": [
    "### Delete Dataproc Cluster\n",
    "\n",
    "When done with the cluster it should be delete to prevent additional costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f10ab061-b44f-40aa-b22b-97362bb71cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_cluster = dataproc_cluster.delete_cluster(\n",
    "    project_id = PROJECT_ID,\n",
    "    region = REGION,\n",
    "    cluster_name = cluster_specs['cluster_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9115c38-0073-44e4-a22e-a23401989dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = delete_cluster.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e96405bf-1388-4032-9222-7ee0259f1b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c958167-8ad1-4b3d-9e3b-84d6164ae6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
