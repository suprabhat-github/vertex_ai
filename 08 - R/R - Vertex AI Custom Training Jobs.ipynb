{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ad4474",
   "metadata": {},
   "source": [
    "![ga4](https://www.google-analytics.com/collect?v=2&tid=G-6VDTYWLKX6&cid=1&en=page_view&sid=1&dl=statmike%2Fvertex-ai-mlops%2F08+-+R&dt=R+-+Vertex+AI+Custom+Training+Jobs.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/08%20-%20R/R%20-%20Vertex%20AI%20Custom%20Training%20Jobs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A//raw.githubusercontent.com/statmike/vertex-ai-mlops/main/08%20-%20R/R%20-%20Vertex%20AI%20Custom%20Training%20Jobs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/R%20-%20Vertex%20AI%20Custom%20Training%20Jobs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A//raw.githubusercontent.com/statmike/vertex-ai-mlops/main/08%20-%20R/R%20-%20Vertex%20AI%20Custom%20Training%20Jobs.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b07c1-4cd3-43e3-8639-feb554954981",
   "metadata": {},
   "source": [
    "# R - Vertex AI Custom Training Jobs\n",
    "\n",
    "Running an **R** script as a job using [Vertex AI Custom Training](https://cloud.google.com/vertex-ai/docs/training/overview).  These allow the specification of a fully manged job with three parts: code, container, compute.\n",
    "\n",
    "Why?\n",
    "- The same script that is being developed in an IDE can be ramped up with much larger data and compute by launching it as a custom training job.\n",
    "- Easily automate the running of jobs by schedule or event triggers\n",
    "- Use larger compute while controlling cost - only pay for the runtime of the job.\n",
    "\n",
    ">This notebook use a Python kernel in order to use the Vertex AI SDK Python Client. To have a complete **R** based workflow, the code in this workflow could be adapted to run in **R** with the [reticulate](https://cran.r-project.org/web/packages/reticulate/vignettes/calling_python.html) package.  This package provides an **R** interface to Python.\n",
    "\n",
    "---\n",
    "Part of the series of [**R**](https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/readme.md) workflows:\n",
    "\n",
    "A series of workflows focused on using **R** in Vertex AI as well as other Google Cloud services to run R code, train models with R, and serve predictionns with R.\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "- This notebook running in Vertex AI Workbench Instance as described in the series [readme](./readme.md)\n",
    "- Run the workflow: [R - Notebook Based Workflow](./R%20-%20Notebook%20Based%20Workflow.ipynb)\n",
    "    - This prepares the data source used by the custom job in this workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bfb8c-bea0-4b10-ad74-f73119e729ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "068b535b-b8c3-4606-855b-78c98bee464e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('google.cloud.storage', 'google-cloud-storage'),\n",
    "    ('google.cloud.artifactregistry_v1', 'google-cloud-artifact-registry'),\n",
    "    ('google.cloud.devtools', 'google-cloud-build')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ad77f-1d25-4651-b3d1-773ccbb64f60",
   "metadata": {},
   "source": [
    "### Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "481ba40d-eb12-4b18-86b4-7bab658129e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable artifactregistry.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365af31-dc11-4a73-a266-e27e9bdd20b2",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4e84dc8-8d67-481c-be36-49f8a8406aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb57dc-1a20-498b-9f3b-6d6bd7e0a80c",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1638565-7009-4342-8df2-4f718cb3bf10",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bac0f7ef-ea88-4270-9ab0-a0c54adb3c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8edfffc7-8c9a-4fed-9973-862979813dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'bigquery-data'\n",
    "SERIES = 'r'\n",
    "\n",
    "# BigQuery Parameters\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES\n",
    "BQ_TABLE = EXPERIMENT\n",
    "BQ_REGION = REGION[0:2]\n",
    "\n",
    "# GCS Parameters: Give bucket name\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "\n",
    "# key columns in the data:\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = ['transaction_id', 'splits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22ad61-085b-4791-8c2f-35863397e52b",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f8c72fc-b735-45aa-8f9a-39487b405a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud.devtools import cloudbuild_v1\n",
    "from google.cloud import artifactregistry_v1\n",
    "from IPython.display import Markdown as md\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5b042-0622-498c-9cd1-9386d43d42af",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b198d96-c7a7-4339-ba99-899e64b3dc83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "URI = f\"gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}\"\n",
    "DIR = f\"temp/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc82ab-b45a-4915-910c-edca3286c662",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e560ce8-e30b-4f87-b652-f1d951dc6b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97e299-193b-4323-aa80-41309d6c2246",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b7d1b6e-a55b-431f-9622-08e82bae72c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c01507-1114-40c7-952c-fc6269b1cd68",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Training Code: **R** Script\n",
    "\n",
    "The prior workflow in this series, [R - Notebook Based Workflow](./R%20-%20Notebook%20Based%20Workflow.ipynb), did the model training work in a notebook using an **R** kernel.  The first step to making this a training job is converting the notebook into an actual **R** script.  \n",
    "\n",
    "The steps from the notebook workflow have been replicated in the **R** script included with this repository.  The cell below loads and shows this script.  \n",
    "- review directly in GitHub with [this link](https://github.com/statmike/vertex-ai-mlops/blob/main/08%20-%20R/code/train.R)\n",
    "\n",
    "**Notes On Script**\n",
    "- The steps are replicated identically with the following additions:\n",
    "    - The parameters are defined as inputs at the top of the code\n",
    "    - the last step of the code uses `saveRDS` to save the model and then uses the automatically setup local path to GCS to copy the model to GCS for further use using `system2` to run a `cp` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "751439d8-055f-49f3-847b-83818211b1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```R\n",
       "\n",
       "# library import\n",
       "library(bigrquery)\n",
       "library(dplyr)\n",
       "\n",
       "# inputs\n",
       "args <- commandArgs(trailingOnly = TRUE)\n",
       "bq_project <- args[1]\n",
       "bq_dataset <- args[2]\n",
       "bq_table <- args[3]\n",
       "var_target <- args[4]\n",
       "var_omit <- args[5]\n",
       "\n",
       "# data source\n",
       "get_data <- function(s){\n",
       "    \n",
       "    # query for table\n",
       "    query <- sprintf('\n",
       "        SELECT * EXCEPT(%s)\n",
       "        FROM `%s.%s.%s`\n",
       "        WHERE splits = \"%s\"\n",
       "    ', var_omit, bq_project, bq_dataset, bq_table, s)\n",
       "    \n",
       "    # connect to table\n",
       "    table <- bq_project_query(bq_project, query)\n",
       "    \n",
       "    # load table to dataframe\n",
       "    return(bq_table_download(table, n_max = Inf))\n",
       "\n",
       "}\n",
       "train <- get_data(\"TRAIN\")\n",
       "test <- get_data(\"TEST\")\n",
       "\n",
       "# logistic regression model\n",
       "model_exp = paste0(var_target, \"~ .\")\n",
       "\n",
       "model <- glm(\n",
       "    as.formula(model_exp),\n",
       "    data = train,\n",
       "    family = binomial)\n",
       "\n",
       "# predictions for evaluation\n",
       "preds <- predict(model, test, type = \"response\")\n",
       "\n",
       "# evaluate\n",
       "actual <- test[, var_target]\n",
       "names(actual) <- 'actual'\n",
       "pred <- tibble(round(preds))\n",
       "names(pred) <- 'pred'\n",
       "results <- cbind(actual, pred)\n",
       "cm <- table(results)\n",
       "\n",
       "# save model to file\n",
       "saveRDS(model, \"model.rds\")\n",
       "\n",
       "# get GCS fusemount location to save file to:\n",
       "path <- sub('gs://', '/gcs/', Sys.getenv('AIP_MODEL_DIR'))\n",
       "#system2('cp', c('model.rds', path))\n",
       "\n",
       "# copy model file to GCS\n",
       "system2('gsutil', c('cp', 'model.rds', Sys.getenv('AIP_MODEL_DIR')))\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a view the script:\n",
    "SCRIPT_PATH = './code/train.R'\n",
    "\n",
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```R\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614930e-4cd8-4605-91ea-9c96ce198f33",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Custom Training Job\n",
    "\n",
    "There are many ways to create a custom training job by combining code with a container.  Check out [this comprehensive review](../Tips/Python%20Training.ipynb) of the possible workflows.\n",
    "\n",
    "This workflow uses a local script, reviewed above, along with a container and compute specification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e2b96-5bbb-43db-ac57-497575c302b4",
   "metadata": {},
   "source": [
    "### Choose Computing Environment\n",
    "\n",
    "When using [custom training on Vertex AI](https://cloud.google.com/vertex-ai/docs/training/custom-training-methods) the compute environment is specified as parameters.  At a minimum this will include the [compute resources](https://cloud.google.com/vertex-ai/docs/training/configure-compute) and [container](https://cloud.google.com/vertex-ai/docs/training/configure-container-settings) URIs.\n",
    "\n",
    "This example uses minimal compute with a single node and no accelerators (GPU).\n",
    "\n",
    "Google Cloud provides prebuilt containers for machine learning that can be used directly or used as the base to build [custom containers](https://cloud.google.com/vertex-ai/docs/training/containers-overview) by adding packages and code with Docker.\n",
    "\n",
    "**Pre-Built Containers Sources**\n",
    "\n",
    "The sources for pre-built containers can be found in these locations:\n",
    "- [Deep Learning Containers With Vertex AI](https://cloud.google.com/vertex-ai/docs/general/deep-learning)\n",
    "    - list these with `gcloud container images list --repository=\"gcr.io/deeplearning-platform-release\"`\n",
    "- [Vertex AI Pre-Built Containers for Custom Training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)\n",
    "    - list these with `gcloud container images list --repository=\"us-docker.pkg.dev/vertex-ai/training\"`\n",
    "- [Vertex AI Pre-Built Containers for Prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)\n",
    "    - list these with `gcloud container images list --repository=\"us-docker.pkg.dev/vertex-ai/prediction\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4de25108-2a59-4330-9b33-e822cc4bb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "TRAIN_IMAGE = 'gcr.io/deeplearning-platform-release/r-cpu.4-3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11babbe-ba85-4d60-8be7-71176c4198c9",
   "metadata": {},
   "source": [
    "### Creating a Custom Container with Cloud Build\n",
    "\n",
    "Cloud Build creates and manages the build on GCP.  The API creates a build by providing:\n",
    "- location of the source\n",
    "- instructions\n",
    "- location to store the built artifacts\n",
    "\n",
    "The instruction part of Cloud Build has options:\n",
    "- Dockerfile\n",
    "- Build Config file (YAML or JSON)\n",
    "- Cloud Native Buildpacks\n",
    "\n",
    "This notebook uses the approach of using the Python Client for Cloud Build and not referencing any local files.  For that reason, the first step is creating a Dockerfile for the workflow and storing it in GCS. The next step is running Cloud Build and using the client to specify the Build config rather than a config file.  The steps of the build config start with getting the code (git clone, or copy from GCS) and copying the Dockerfile.  \n",
    "\n",
    "There are many workflows for creating containers with ML training code.  Many of the most common ones are explored in the tips notebook [Python Custom Containers](../Tips/Python%20Custom%20Containers.ipynb).  The method used here is the simplest - copy the training code directly into the container.  The other methods include packaging the training code as a Python Distribution and using `pip install` in from GCS, GitHub and even Artifact Registry as a private repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87ef84-594c-47ca-b15b-20bee082fc10",
   "metadata": {},
   "source": [
    "#### Store Resources in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "48ca87fe-8632-4021-a2a7-65a649e1f726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(GCS_BUCKET)\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbb4a0-60d7-444c-99f5-949389171d7f",
   "metadata": {},
   "source": [
    "#### Copy Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74ed8d4a-cd64-4774-abed-cd000395bf7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/train.R')\n",
    "blob.upload_from_filename(SCRIPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f28fbe-99f9-407f-bf15-dc3f71dd34d3",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "A basic dockerfile thats take the base image and copies the code in and define an entrypoint - what python script to run first in this case.  Add RUN entries to pip install additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fdf4f0dc-face-4dba-a654-15c0eadada95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {TRAIN_IMAGE}\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# copy code into /code folder:\n",
    "COPY ./*.R ./code/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b4262e54-bc39-4d1a-9958-eac719ba6575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/Dockerfile')\n",
    "blob.upload_from_string(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca7c9d-87b9-4df6-a6e0-fd2179ddef04",
   "metadata": {},
   "source": [
    "#### Setup Artifact Registry\n",
    "\n",
    "Artifact registry organizes artifacts with repositories.  Each repository contains packages and is designated to hold a partifcular format of package: Docker images, Python Packages and [others](https://cloud.google.com/artifact-registry/docs/supported-formats#package)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47b590-b43b-4a21-839c-e9b45f5e1ee2",
   "metadata": {},
   "source": [
    "##### List Repositories\n",
    "\n",
    "This may be empty if no repositories have been created for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd4e334b-d982-4ecd-b4a4-39d9ea3de7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb102721-a265-443a-b68a-60ba1d227429",
   "metadata": {},
   "source": [
    "#### Create Docker Image Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Docker Images created by this notebook.  First, check to see if it is already created by a previous run and retrieve it if it has.  Otherwise, create!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7a94881-2551-4065-8b23-5fd1e3aaddbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing repo: projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "docker_repo = None\n",
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    if f'{PROJECT_ID}' == repo.name.split('/')[-1]:\n",
    "        docker_repo = repo\n",
    "        print(f'Retrieved existing repo: {docker_repo.name}')\n",
    "\n",
    "if not docker_repo:\n",
    "    operation = ar_client.create_repository(\n",
    "        request = artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "            repository_id = f'{PROJECT_ID}',\n",
    "            repository = artifactregistry_v1.Repository(\n",
    "                description = f'A repository for the {SERIES} series that holds docker images.',\n",
    "                name = f'{PROJECT_ID}',\n",
    "                format_ = artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels = {'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('Creating Repository ...')\n",
    "    docker_repo = operation.result()\n",
    "    print(f'Completed creating repo: {docker_repo.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9a89d3c0-b04a-4321-af3b-e9aaf728fa8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915',\n",
       " 'DOCKER')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_repo.name, docker_repo.format_.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2244069a-8edc-4d67-8ec4-7df8924604c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPOSITORY = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{docker_repo.name.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ba869-7444-4037-b5e5-6d27112f8e3e",
   "metadata": {},
   "source": [
    "#### Build Custom Container\n",
    "Use the Cloud Build client to construct and run the build instructions.  Here the files collected in GCS are copied to the build instance, then the Docker build is run in the folder with the `Dockerfile`.  The resulting image is pushed to Artifact Registry (setup above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6f00b3ad-2a65-4a93-883d-5c96516c436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the build config with empty list of steps - these will be added sequentially\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps = []\n",
    ")\n",
    "# retrieve the source\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/gsutil',\n",
    "        'args': ['cp', '-r', f'gs://{GCS_BUCKET}/{SOURCEPATH}/*', '/workspace']\n",
    "    }\n",
    ")\n",
    "# docker build\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/docker',\n",
    "        'args': ['build', '-t', f'{REPOSITORY}/{SERIES}_{EXPERIMENT}_trainer', '/workspace']\n",
    "    }    \n",
    ")\n",
    "# docker push\n",
    "build.images = [f\"{REPOSITORY}/{SERIES}_{EXPERIMENT}_trainer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b7733891-1695-4c85-be6a-cb8edb19a5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/gsutil\"\n",
       "  args: \"cp\"\n",
       "  args: \"-r\"\n",
       "  args: \"gs://statmike-mlops-349915/r/bigquery-data/models/20240127154845/*\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/docker\"\n",
       "  args: \"build\"\n",
       "  args: \"-t\"\n",
       "  args: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/r_bigquery-data_trainer\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/r_bigquery-data_trainer\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "518373c7-ec28-4fc0-9ca9-87a194f59afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "operation = cb_client.create_build(\n",
    "    project_id = PROJECT_ID,\n",
    "    build = build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c39ade5b-9f9c-41b9-99f4-aae7a486cc37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Status.SUCCESS: 3>,\n",
       " images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/r_bigquery-data_trainer\")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = operation.result()\n",
    "response.status, response.artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c03e07f2-a66b-4027-98b1-7e3b452dec6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Container with Artifact Registry in the Google Cloud Console:\n",
      "https://console.cloud.google.com/artifacts/docker/statmike-mlops-349915/us-central1/statmike-mlops-349915?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review the Custom Container with Artifact Registry in the Google Cloud Console:\\nhttps://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{PROJECT_ID}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520b858-75ef-4bc4-9a38-4fd30cc8169b",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bef5d7b6-4876-4c73-bf72-7e2cd5187069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    BQ_PROJECT,\n",
    "    BQ_DATASET,\n",
    "    BQ_TABLE,\n",
    "    VAR_TARGET,\n",
    "    ','.join(VAR_OMIT)\n",
    "]\n",
    "\n",
    "MACHINE_SPEC = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_count\": 0\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": MACHINE_SPEC,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": f\"{REPOSITORY}/{SERIES}_{EXPERIMENT}_trainer\",\n",
    "            \"command\": [\"Rscript\", \"./code/train.R\"],\n",
    "            \"args\": CMDARGS\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b823024-2aec-40d5-8401-fae7783469fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomJob(\n",
    "    display_name = f'{SERIES}_{EXPERIMENT}_{TIMESTAMP}',\n",
    "    worker_pool_specs = WORKER_POOL_SPEC,\n",
    "    base_output_dir = f\"{URI}/models/{TIMESTAMP}\",\n",
    "    staging_bucket = f\"{URI}/models/{TIMESTAMP}\",\n",
    "    labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'timestamp' : f'{TIMESTAMP}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab0293-c1c0-4536-8c0c-163de8a5362b",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5cd74777-a9fb-4947-a0a9-19d64e6d0205",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/1026793852137/locations/us-central1/customJobs/2444232421768429568\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/1026793852137/locations/us-central1/customJobs/2444232421768429568')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2444232421768429568?project=1026793852137\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_QUEUED\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1026793852137/locations/us-central1/customJobs/2444232421768429568 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/1026793852137/locations/us-central1/customJobs/2444232421768429568\n"
     ]
    }
   ],
   "source": [
    "customJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "965366bf-0116-454e-94db-65c75a95dadc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r_bigquery-data_20240127154845'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customJob.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "650a50f3-acf6-4971-9175-981cfc953dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1026793852137/locations/us-central1/customJobs/2444232421768429568'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customJob.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c97d1-8acf-43e9-ac48-fb9b684edf66",
   "metadata": {},
   "source": [
    "Create hyperlinks to job and tensorboard here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bad013f5-239c-46cf-8d65-ebec0f2c1ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Job here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/training/2444232421768429568/cpu?cloudshell=false&project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "job_link = f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/training/{customJob.resource_name.split('/')[-1]}/cpu?cloudshell=false&project={PROJECT_ID}\"\n",
    "\n",
    "print(f'Review the Custom Job here:\\n{job_link}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945044e-7915-4a6d-889a-38d2a1027af5",
   "metadata": {},
   "source": [
    "### Review Files In GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b64cbbb3-4f69-4b99-b74c-c037129239d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the files in GCS here:\n",
      "https://console.cloud.google.com/storage/browser/statmike-mlops-349915/r/bigquery-data/models/20240127154845?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review the files in GCS here:\\nhttps://console.cloud.google.com/storage/browser/{GCS_BUCKET}/{SOURCEPATH}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b69d961b-107d-48d6-adf1-c7e8e0304e67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Blob: statmike-mlops-349915, r/bigquery-data/models/20240127154845/Dockerfile, 1706373769111426>,\n",
       " <Blob: statmike-mlops-349915, r/bigquery-data/models/20240127154845/model/model.rds, 1706374211615633>,\n",
       " <Blob: statmike-mlops-349915, r/bigquery-data/models/20240127154845/train.R, 1706373768502733>]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bucket.list_blobs(prefix = SOURCEPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9232a8-91d4-4907-a58b-01d25f986589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf185e-989e-447f-adbb-26a5ee2d6825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683c2d5-0c3d-48d3-82f5-1b774caf5285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46eaf0-782f-4597-85a3-eb554e8d02ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c255d-70d4-4a75-8876-48f237899dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
